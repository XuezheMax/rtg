[#conf.yml]
== RTG *`conf.yml`* File

The key component of RTG toolkit is a `conf.yml`. As the name says - it is a YAML file containing configuration
for an experiment.
Before we get into understanding what goes into configuration file

* Experiment - the top level entity that wraps everything below, for the sake of reproducibility.
* Data Preparation - NLP datasets require preparation of textual data. Typically, creation of
vocabulary to map text into sequence of integers. Here we can specify type of encoding scheme
such as BPE/char/words, and vocabulary size.
* Model - model is neural net for NMT or LM tasks. Here we
* Optimizer - Optimizer and optimization criteria
* Trainer - training steps, batch size etc
* Tester [Optional] -- testing to do post training
** Tuner [Optional] - to search for beam size, length penalty etc
** Decoder - the Beam decoder parameters, maybe overwritten by Tuner
** Suite - a set of source and reference file pairs, for computing BLEU scores


=== Minimal Yet Complete Config File:

.conf.yml
[source,yaml]
----
model_args: # model construction args
  ff_size: 2048
  hid_size: 512
  n_heads: 8
  attn_dropout: 0.1  # Use lower dropout rates for attention because it masks an entire timestep 
  dropout: 0.2
  enc_layers: 6
  dec_layers: 6
  src_vocab: 8000
  tgt_vocab: 8000
  tied_emb: three-way  # choices: null, one-way, two-way, three-way
model_type: tfmnmt  # model type. tfmnmt is the transformer NMT model
optim:
  name: ADAM
  args:
    criterion: smooth_kld  # other choices: "cross_entropy", "binary_cross_entropy",
                           #                "smooth_kld_and_triplet_loss",
                           #                "triplet_loss" (doesn't converge alone)
    betas:
      - 0.9
      - 0.98
    eps: 1.0e-09
    label_smoothing: 0.1
    lr: 0.2
    warmup_steps: 8000
    constant: 2
    inv_sqrt: false        # False = default NoamOpt learning rate schedule
    amsgrad: false
    weight_decay: 0
prep: # data preparation
  max_types: 8000  # maximum number of types in vocab ; if shared_vocab=false, set max_src_types and max_tgt_types separately
  pieces: bpe   # choices: bpe, char, word, unigram  from google/sentencepiece
  shared_vocab: true  # true means same vocab for src and tgt, false means different vocabs
  src_len: 256   # longer sentences, decision is made as per 'truncate={true,false}'
  tgt_len: 256
  truncate: true  # what to do with long sentences: if true truncate at src_len or tgt_len; if false filter away
  train_src: wmt_data/data/de-en/europarl-v9.de-en.de.tok   # training data
  train_tgt: wmt_data/data/de-en/europarl-v9.de-en.en.tok
  valid_src: wmt_data/data/dev/newstest2013.de.tok
  valid_tgt: wmt_data/data/dev/newstest2013.en.tok
tester:
  decoder:
   beam_size: 4
   batch_size: 18000   # effective size = batch_size/beam_size
  suit:  # suit of tests to run after the training
    newstest2013:  # name of test and list of src.tok, ref file (ref should be unmodified)
      - wmt_data/data/dev/newstest2013.de.tok
      - wmt_data/data/dev/newstest2013.en
    newstest2014:  # name of test and list of src.tok, ref file (ref should be unmodified)
      - wmt_data/data/dev/newstest2014-deen-src.de.tok
      - wmt_data/data/dev/newstest2014-deen-ref.en
trainer:
  init_args:
    chunk_size: 10   # generation in chunks of time steps to reduce memory consumption
    grad_accum: 1     # How many batches to accumulate gradients
  batch_size: 4200   # not exceeding these many tokens (including paddings)
  check_point: 1000  # how often to checkpoint?
  keep_models: 10   # how many checkpoints to keep on disk (small enough to save disk, large enough for checkpt averaging
  steps: 200000      # how many steps to train; if early_stop is enabled, this is max steps
  keep_in_mem: True   # keep training data in memory
updated_at: '2019-03-09T21:15:33.707183'  # automatically updated by system
seed: 12345  # fix the manual seed of pytorch + cuda + numpy + python_stdlib RNGs. Remove/comment this to disable
----

=== Early stop
Add the below piece of config to `trainer` to enable early stop on convergence.
[source,yaml]
----
trainer:
  ....           # other args
  steps: 100000      # steps is treated as max steps
  checkpoint: 1000   # validate every these many steps
  early_stop:       # remove this block to disable
    enabled: true   # or, alternatively flip this to disable;
    by: loss        # stop by validation loss (default); TODO: add BLEU
    patience: 5     # how many validations to wait, to be sure of stopping; each validation is per check_point steps
    min_steps: 8000  # minimum steps to wait before test for early stop;
    signi_round: 3   # significant in 'by' value, used as round(value, signi_round).
                     # e.g. round(1/3, 3) = 0.333; round(100/3, 0) = 33; round(100/3, -1) = 30.0

----

=== Optimizer

By default, we use the `ADAM` optimizer from
link:https://arxiv.org/abs/1412.6980[Adam: A Method for Stochastic Optimization].
It is also possible to use `ADAMW` from link:https://arxiv.org/abs/1711.05101[Decoupled Weight Decay Regularization],
since weight decay is different in optimizers with variable step sizes.

Note: When `inv_sqrt: false`, `lr` does nothing. When `inv_sqrt: true`, `constant` does nothing.

An alternative optimizer may look like:
[source,yaml]
----
optim:
  name: ADAMW
  args:
    criterion: smooth_kld_and_triplet_loss
    betas:
      - 0.9
      - 0.98
    eps: 1.0e-08
    label_smoothing: 0.1
    lr: 0.0005
    warmup_steps: 4000
    constant: 2           # Does nothing if inv_sqrt==True
    inv_sqrt: true        # If truem lr should be under 0.005
    amsgrad: true
    weight_decay: 1e-3    # Use ADAMW if weight decay != 0
----

=== Fine Tuning

We define fine tuning as the act of  changing the training data at certain time step in the training process.
To enable this feature, we need to do following.

Step1. specify, `finetune_src` `finetune_tgt` in the `prep` block as follows
[source,yaml]
----
prep: # data preparation
  ....
  train_src: wmt_data/data/de-en/europarl-v9.de-en.de.tok   # training data
  train_tgt: wmt_data/data/de-en/europarl-v9.de-en.en.tok
  finetune_src: wmt_data/data/de-en/finetune.de-en.de.tok   # Finetuning data
  finetune_tgt: wmt_data/data/de-en/finetune.de-en.en.tok
  valid_src: wmt_data/data/dev/newstest2013.de.tok
  valid_tgt: wmt_data/data/dev/newstest2013.en.tok
----
Step2, Inform the Trainer to continue training, edit the `trainer` block with `finetune_steps`.
[source,yaml]
----
trainer:
  batch_size: 12000        # training batch size
  steps: 200000           # how many steps to train
  finetune_steps: 300000 # fine tuning steps.
  finetune_batch_size: 1024  # fine tuning batch_size; optional; default is training batach_size

----
This makes the trainer use `train_{src,tgt}` for 0 - 200k steps,  followed by `finetune_{src,tgt}`
for 200k-300k steps. Note that `finetune_steps > steps` .

=== Parent-Child : using pretrained model as parent
To initialize from another compatible model as parent, add `parent:` specification to conf.yml as shown below:
[source,yaml]
----
model_type: tfmnmt
model_args:
  # will be inherited from parent  ; see parent.mode.args: true
parent:
  experiment: <path/to/experiment/dir>
  vocab:
    shared: shared       # for reusing the shared vocab
    #src: src            # for separate vocabs
    #tgt: tgt
  model:
    args: true          # update/overwrite the model_args of child with the parent
    ensemble: 5         # how many checkpoints of parent to ensemble, to obtain initial state
# ... rest of the config such as prep, trainer etc
----

=== Freezing some parts of model
Frozen weights associated to parts of network means the weights remain unmodified during the course of the training.
It is a useful feature when the model weights are initialized from a well trained parent model.
WKT Optimizer is the one that modifies model's parameters according to their gradients.
Therefore, to freeze the weights implies excluding the weights from optimizer.
Or alternatively, explicitly mention the parts of the model needs to be trained (i.e. updated by optimizer).

Here is an example -- comment or remove the parts that you wish to freeze in the below 6 layer network.
[source,yaml]
----
optim:
  name: ADAM
  args:
    ....# the usual args for optimizer
  trainable:  # trainable parameter
    include: # only include these and exclude everything else not listed here
    - src_embed
    - tgt_embed
    - generator
    - 'encoder:0,1,2,3,4,5'  # the numbers are layer indices starting from 0
    - 'decoder:0,1,2,3,4,5'  # the numbers are layer indices starting from 0
----
TODO: add support for `exclude` logic ie. include everything else except the mentioned.

This feature is supported only in `AbstractTransformerNMT` and all of its children.
If you are adding a new `NMTModel` or customising this feature, please override `get_trainable_parameters(self, include, exclude)` function to support this feature.

=== Sharing the data from other experiment

In the new experiment config, add `same_data` to reference parent experiment from which the data
should be reused for training and validation. Note that this uses the same vocabulary as parent.
The child experiment creates a symbolic link to parent experiments data (instead of copying,
to reduce the disk space).

[source,yaml]
----

prep:
  same_data: path/to/prior/experiment_dir

----

== Vocabulary Preprocessing using Sentencepiece or NLCodec

link:https://github.com/google/sentencepiece[Google's sentencepiece] is an awesome lib for
preprocessing the text datasets.
We've used sentencepiece's python API since day-1 of RTG and it is the default library.
However, since the core sentencepiece is written in C++, it was hard to modify to explore some new
ideas on BPE (without knowing C++). So, we reimplemented BPE in pure python, with advanced
datastructures such as linked-lists, prefix tries and dirty-maxheap to match the speed.
Our reimplementation is named as link:https://github.com/isi-nlp/nlcodec/[NLCodec].
NLCoded can be enabled as:

[source, yaml]
----
prep:
  ....
  codec_lib: nlcodec  # default is sentpiece
----

=== Vocabulary Types
Both `sentpiece` or `nlcodec` support `pieces=` `bpe`, `char`, `word`.

[source, yaml]
----
prep:
  ....
  codec_lib: nlcodec  # other option: sentpiece
  pieces: bpe         # other options: char, word
----
As of now, only `sentpiece` supports `pieces=unigram`.

=== Character coverage

For `bpe` and `char` vocabulary types, a useful trick is to exclude low frequency character and mark them as `UNK's`.
Usually expressed as percentage of character coverage in training corpus.
Sentencepiece's default (when we last checked) is 99.95% ie 0.9995.
Here is how to set this for eg to 99.99% i.e. 0.9999 in `nlcodec`
[source, yaml]
----
prep:
  ....
  codec_lib: nlcodec  # other option: sentpiece
  pieces: bpe         # other options: char, word
  char_coverage: 0.9999         # other options: char, word
----

