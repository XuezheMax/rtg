== Scaling to Big Datasets Using PySpark

When dealing with big datasets, the traditional tools such as multiprocessing and SQLite3 simply aren't enogh.
In such scenario, https://spark.apache.org/[PySpark] is a useful backend to use.
When pyspark is enabled

* https://parquet.apache.org/[Parquet] storage is used instead of https://www.sqlite.org/index.html[SQLite3] to store the training dataset.
* PySpark is used to
 ** compute term frequencies which help speed up BPE learning
 ** shuffle dataset for every epoch.

To enable pyspark backend

1. Install pyspark; eg: `pip install pyspark >= 3.0.0`.  Make sure you have a JVM/JDK that is compatible for pyspark.
2. For data preparation, only `codec_lib: nlcodec` supports pyspark backend as of now. If you are using `sentencepiece`, switch to `nlcodec`
3. Add the block to the top level of `conf.yml`.  See `experiments/transformer.test.spark.yaml` for a full example.

[source,yaml]
----
prep:
   codec_lib: nlcodec    # only nlcodec is supports pyspark backend
   ... # other args
spark: # add this block to enable spark backend
  # double quote the keys containing dot
  "spark.master": local[3]           # set it to local[*] to use all local CPUs
  "spark.app.name": RTG NMT on Spark  # Name for the App
  "spark.driver.memory": 6g
  #key1: value1    # any other spark configs you want to control
  len_sort_size: 20000  # keep these recs in memory for making equal length batches
----

You may use `local[*]` to use all CPUs however, it is important to note that:

1. If you have too many CPU cores (say 32 or more), the disk/storage may have too much pressure and the overall performance could degrade.
2. Remember to exclude some CPUs out from spark for other work load, such as for pytorch. In the above example I used `local[3]` because I had only 4 CPUs in total and excluded one from spark.

Watch out the spark logs for any warning messages.
Also, the log message provides the Spark web UI address when spark session is initialized.

WARNING: The spark distributed mode is not tested. (But it might work out of the box  if `"spark.master"` is correctly set)


=== Postgres backend for dataset

To draw IID examples to training process, the dataset backend needs fast random access to examples.
Spark RDDs/Dataframes are not meant for random access, so we need something of a database that builds ahead-of-time index rows.

We already have sqlite3 for that reason, however, Sqlite3 doesnt scale well to

Postgres can be easily installed on many platforms (including PPC64) using conda.
Sudo/root is not needed to setup. The instructions are as follows:

[source,bash]
----
# install
conda install -c anaconda postgresql

# setup db dir under <myexp>;  for simplicity, each expe gets own PostgresDB
pg_ctl init -D <myexp>/pg-data -U rtg

pg_ctl start -D <myexp>/data-pg
----

WARNING:  this is incomplete
