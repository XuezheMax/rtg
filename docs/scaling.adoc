== Scaling to Big Datasets Using PySpark

When dealing with big datasets, the traditional tools such as multiprocessing and SQLite3 simply aren't enogh.
In such scenario, https://spark.apache.org/[PySpark] is a useful backend to use.
When pyspark is enabled

PySpark is used to

* compute term frequencies which help speed up BPE learning
* encode data with BPE
* store data in https://isi-nlp.github.io/nlcodec/#_database[NLCodec MultipartDb  ]


To enable pyspark backend

1. Install pyspark; eg: `pip install pyspark >= 3.0.0`.  Make sure you have a JVM/JDK that is compatible for pyspark.
2. For data preparation, only `codec_lib: nlcodec` supports pyspark backend as of now. If you are using `sentencepiece`, switch to `nlcodec`
3. Add the `spark` block to the top level of `conf.yml`.  See `experiments/spark-bigdataprep.html` for a full example.

[source,yaml]
----
prep:
   codec_lib: nlcodec    # only nlcodec supports pyspark backend
   max_part_size: 1000000  # part size (num of recs); divides the training data into multiple parts
   ... # other args
spark: # add this block to enable spark backend
  # double quote the keys containing dot
  "spark.master": local[3]           # set it to local[*] to use all local CPUs
  "spark.app.name": RTG NMT on Spark  # Name for the App
  "spark.driver.memory": 6g
  #key1: value1    # any other spark configs you want to control

----

You may use `local[*]` to use all CPUs however, it is important to note that:

1. If you have too many CPU cores (say 32 or more), the disk/storage may have too much pressure and the overall performance could degrade.
2. Remember to exclude some CPUs out from spark for other work load, such as for pytorch. In the above example I used `local[3]` because I had only 4 CPUs in total and excluded one from spark.

Watch out the spark logs for any warning messages.
Also, the log message provides the Spark web UI address when spark session is initialized.

WARNING: The multi-node spark distributed mode is not tested. (But it might work out of the box  if `"spark.master"` is correctly set)


[#ddp]
== Distributed Data Parallel (DDP)

NOTE: This is a new feature to RTG and not all edge cases are tested.

`rtg.distrib.launch` simplifies the use of `torch.distributed.launch` as follows:

[source,bash]
----
$ python -m rtg.distrib.launch -h
usage: launch.py [-h] [-N NODES] [-r NODE_RANK] [-P PROCS_PER_NODE]
                 [-G GPUS_PER_PROC] [--master-addr MASTER_ADDR]
                 [--master-port MASTER_PORT] [-m | --no_python]
                 training_script ...

PyTorch distributed training launch helper utilty that will spawn up multiple
distributed processes

positional arguments:
  training_script       The full path to the single GPU training
                        program/script to be launched in parallel, followed by
                        all the arguments for the training script
  training_script_args

optional arguments:
  -h, --help            show this help message and exit
  -N NODES, --nodes NODES
                        The number of nodes to use for distributed training
                        (default: 1)
  -r NODE_RANK, --node-rank NODE_RANK
                        The rank of the node for multi-node distributed
                        training (default: 0)
  -P PROCS_PER_NODE, --procs-per-node PROCS_PER_NODE
                        The number of processes to launch on each node with
                        one gpu each, for GPU training, this is recommended to
                        be set to the number of GPUs in your system so that
                        each process can be bound to a single GPU. (default:
                        1)
  -G GPUS_PER_PROC, --gpus-per-proc GPUS_PER_PROC
                        Number of GPUs to assign to each process. (default: 0)
  --master-addr MASTER_ADDR
                        Master node (rank 0)'s address, should be either the
                        IP address or the hostname of node 0, for single node
                        multi-proc training, the --master_addr can simply be
                        127.0.0.1 (default: 127.0.0.1)
  --master-port MASTER_PORT
                        Master node (rank 0)'s free port that needs to be used
                        for communciation during distributed training
                        (default: 29500)
  -m, --module          Changes each process to interpret the launch script as
                        a python module, executing with the same behavior
                        as'python -m'. (default: False)
  --no_python           Do not prepend the training script with "python" -
                        just exec it directly. Useful when the script is not a
                        Python script. (default: False)

----

**Examples**

. Run on two CPU processes `-P 2` on single node `-N 1` (for testing, no GPUS `-G 0`)
+
----
python -m rtg.distrib.launch -N 1 -P 2 -G 0 -m rtg.pipeline  runs/005-tfm-nldb
----
. Run on on single node, two processes, one GPU per process: `-N 1 -P 2 -G 1`
. Run on on two node, two processes each, one GPU per process: `-N 2 -P 2 -G 1`.
+
[source,bash]
----
# on first node: rank 0
python -m rtg.distrib.launch -N 2 -r 0 -P 2 -G 1 -m rtg.pipeline runs/005-tfm-nldb -G
# on second node: rank 1
python -m rtg.distrib.launch -N 2 -r 1 -P 2 -G 1 -m rtg.pipeline  runs/005-tfm-nldb -G
----

WARNING:

1. ChunkedLossCumpute doesnt work with DDP on GPUs; please disable it by `trainer.init_args.args.chunk_size=0`
2. Don't ever use `-G 2` or more, instead use more `-P`