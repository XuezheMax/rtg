model_args:
  src_lang: FRA
  tgt_lang: ENG
  n_layers: 2
  emb_size: 300
  hid_size: 300
  src_vocab: 8000
  tgt_vocab: 8000

model_type: seq2seq
optim:
  name: ADAM
  warmup_steps: 2000
trainer_args:
  steps: 128000
  init_emb:
    - src
    - tgt_in
    - tgt_out

prep:
  mono_src: []
  mono_tgt: []
  src_len: 100
  text_pieces: false
  tgt_len: 100
  train_src: data/train.src
  train_tgt: data/train.tgt
  truncate: true
  valid_src: data/valid.src
  valid_tgt: data/valid.tgt
  shared_vocab: true
  max_types: 8000
  pieces: unigram
  finetune_src: data/finetune.src
  finetune_tgt: data/finetune.tgt
  pre_emb_src: data/vectors.src.txt
  pre_emb_tgt: data/vectors.tgt.txt
src_lang: FRA
tgt_lang: ENG
updated_at: '2018-07-25T15:50:52.578881'

