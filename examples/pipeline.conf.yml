model_args:
  dropout: 0.1
  ff_size: 512 # change to 2048
  hid_size: 256  # change to 512
  n_heads: 4    # change to 8
  n_layers: 3   # change to 6
  src_vocab: 8000
  tgt_vocab: 8000
  tied_emb: three-way
model_type: tfmnmt
optim:
  args:
    betas:
    - 0.9
    - 0.98
    eps: 1.0e-09
    label_smoothing: 0.1
    lr: 0.1
    warmup_steps: 4000
  name: ADAM
prep:
  max_types: 500  # change to 8000
  pieces: bpe
  shared_vocab: true
  src_len: 500
  tgt_len: 500
  train_src: data/train.src
  train_tgt: data/train.tgt
  truncate: false
  valid_src: data/valid.src
  valid_tgt: data/valid.tgt
tester:
  decoder:
    beam_size: 4
    max_len: 60
  suit:
    valid:
    - data/valid.src
    - data/valid.tgt
trainer:
  init_args:
    chunk_size: 10   # generation in chunks of time steps to reduce memory consumption
  batch_size: 1024   # tokens
  check_point: 400  # change to 1000 or 2000
  keep_models: 10
  steps: 2000   # change to 128000 or up
updated_at: '2019-03-09T21:15:33.707183'
