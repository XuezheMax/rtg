{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting to know LSTMs better\n",
    "\n",
    "Created: September 13, 2018  \n",
    "Author: Thamme Gowda  \n",
    "\n",
    "\n",
    "Goals:\n",
    "- To get batches of *unequal length sequences* encoded correctly!\n",
    "- Know how the hidden states flow between encoders and decoders\n",
    "- Know how the multiple stacked LSTM layers pass hidden states\n",
    "\n",
    "Example: a simple bi-directional LSTM which takes 3d input vectors\n",
    "and produces 2d output vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(3, 2, batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Input:\n",
      "tensor([[[0.7850, 0.6658, 0.7522],\n",
      "         [0.3855, 0.7981, 0.6199],\n",
      "         [0.9081, 0.6357, 0.3619],\n",
      "         [0.2481, 0.5198, 0.2635]],\n",
      "\n",
      "        [[0.2654, 0.9904, 0.3050],\n",
      "         [0.1671, 0.1709, 0.2392],\n",
      "         [0.0705, 0.4811, 0.3636],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.6474, 0.5172, 0.0308],\n",
      "         [0.5782, 0.3083, 0.5117],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]]])\n",
      "Sequence Lenghts:  [4, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "# Lets create a batch input.\n",
    "# 3 sequences in batch (the first dim) , see batch_first=True\n",
    "# Then the logest sequence is 4 time steps, ==> second dimension\n",
    "# Each time step has 3d vector which is input ==> last dimension\n",
    "pad_seq = torch.rand(3, 4, 3)\n",
    "\n",
    "# That is nice for the theory\n",
    "# but in practice we are dealing with un equal length sequences\n",
    "# among those 3 sequences in the batch, lets us say \n",
    "# first sequence is the longest, with 4 time steps --> no padding needed\n",
    "# second seq is 3 time steps --> pad the last time step\n",
    "pad_seq[1, 3, :] = 0.0\n",
    "# third seq is 2 time steps --> pad the last two steps\n",
    "pad_seq[2, 2:, :] = 0.0\n",
    "print(\"Padded Input:\")\n",
    "print(pad_seq)\n",
    "\n",
    "# so we got these lengths\n",
    "lens = [4,3,2]\n",
    "\n",
    "print(\"Sequence Lenghts: \", lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Outputs:\n",
      "tensor([[[ 0.0428, -0.3015,  0.0359,  0.0557],\n",
      "         [ 0.0919, -0.4145,  0.0278,  0.0480],\n",
      "         [ 0.0768, -0.4989,  0.0203,  0.0674],\n",
      "         [ 0.1019, -0.4925, -0.0177,  0.0224]],\n",
      "\n",
      "        [[ 0.0587, -0.3025,  0.0017,  0.0201],\n",
      "         [ 0.0537, -0.3388, -0.0532,  0.0111],\n",
      "         [ 0.0839, -0.3811, -0.0446, -0.0020],\n",
      "         [ 0.0595, -0.3681, -0.0720,  0.0218]],\n",
      "\n",
      "        [[ 0.0147, -0.2585, -0.0093,  0.0756],\n",
      "         [ 0.0398, -0.3531, -0.0174,  0.0369],\n",
      "         [ 0.0458, -0.3476, -0.0912,  0.0243],\n",
      "         [ 0.0422, -0.3360, -0.0720,  0.0218]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# lets send padded seq to LSTM\n",
    "out,(h_t, c_t) = lstm(pad_seq)\n",
    "print(\"All Outputs:\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^ Output is 2x2d=4d vector since it is bidirectional  \n",
    "forward 2d, backward 2d are concatenated  \n",
    "Total vectors=12: 3 seqs in batch x 4 time steps;;  each vector is 4d    \n",
    "\n",
    "> Hmm, what happened to my padding time steps? Will padded zeros mess with the internal weights of LSTM when I do backprop?\n",
    "\n",
    "---\n",
    "Lets look at the last Hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1019, -0.4925],\n",
      "         [ 0.0595, -0.3681],\n",
      "         [ 0.0422, -0.3360]],\n",
      "\n",
      "        [[ 0.0359,  0.0557],\n",
      "         [ 0.0017,  0.0201],\n",
      "         [-0.0093,  0.0756]]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(h_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last hidden state is a 2d (same as output) vectors,  \n",
    "but 2 for each step because of bidirectional rnn  \n",
    "There are 3 of them since there were three seqs in the batch  \n",
    "each corresponding to the last step  \n",
    "But the definition of *last time step* is bit tricky  \n",
    "For the left-to-right LSTM, it is the last step of input  \n",
    "For the right-to-left LSTM, it is the first step of input  \n",
    "\n",
    "This makes sense now.\n",
    "\n",
    "--- \n",
    "Lets look at $c_t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last c_t:\n",
      "tensor([[[ 0.3454, -1.0070],\n",
      "         [ 0.1927, -0.6731],\n",
      "         [ 0.1361, -0.6063]],\n",
      "\n",
      "        [[ 0.1219,  0.1858],\n",
      "         [ 0.0049,  0.0720],\n",
      "         [-0.0336,  0.2787]]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Last c_t:\")\n",
    "print(c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be similar to the last hidden state.\n",
    "\n",
    "\n",
    "## Question: \n",
    "> what happened to my padding time steps? Did the last hidden state exclude the padded time steps?\n",
    "\n",
    "I can see that last hidden state of the forward LSTM didnt distinguish padded zeros. \n",
    "\n",
    "Lets see output of each time steps and last hidden state of left-to-right LSTM, again.   \n",
    "We know that the lengths (after removing padding) are \\[4,3,2]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All time stamp outputs:\n",
      "tensor([[[ 0.0428, -0.3015],\n",
      "         [ 0.0919, -0.4145],\n",
      "         [ 0.0768, -0.4989],\n",
      "         [ 0.1019, -0.4925]],\n",
      "\n",
      "        [[ 0.0587, -0.3025],\n",
      "         [ 0.0537, -0.3388],\n",
      "         [ 0.0839, -0.3811],\n",
      "         [ 0.0595, -0.3681]],\n",
      "\n",
      "        [[ 0.0147, -0.2585],\n",
      "         [ 0.0398, -0.3531],\n",
      "         [ 0.0458, -0.3476],\n",
      "         [ 0.0422, -0.3360]]], grad_fn=<SliceBackward>)\n",
      "Last hidden state (forward LSTM):\n",
      "tensor([[ 0.1019, -0.4925],\n",
      "        [ 0.0595, -0.3681],\n",
      "        [ 0.0422, -0.3360]], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"All time stamp outputs:\")\n",
    "print(out[:, :, :2])\n",
    "print(\"Last hidden state (forward LSTM):\")\n",
    "print(h_t[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Okay, Now I get it.* \n",
    "When building sequence to sequence (for Machine translation) I cant pass last hidden state like this to a decoder.\n",
    "\n",
    "We have to inform the LSTM about lengths.\n",
    "\n",
    "How? \n",
    "\n",
    "Thats why we have `torch.nn.utils.rnn.pack_padded_sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Seqs:\n",
      "tensor([[[0.7850, 0.6658, 0.7522],\n",
      "         [0.3855, 0.7981, 0.6199],\n",
      "         [0.9081, 0.6357, 0.3619],\n",
      "         [0.2481, 0.5198, 0.2635]],\n",
      "\n",
      "        [[0.2654, 0.9904, 0.3050],\n",
      "         [0.1671, 0.1709, 0.2392],\n",
      "         [0.0705, 0.4811, 0.3636],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.6474, 0.5172, 0.0308],\n",
      "         [0.5782, 0.3083, 0.5117],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]]])\n",
      "Lens: [4, 3, 2]\n",
      "Pack Padded Seqs:\n",
      "PackedSequence(data=tensor([[0.7850, 0.6658, 0.7522],\n",
      "        [0.2654, 0.9904, 0.3050],\n",
      "        [0.6474, 0.5172, 0.0308],\n",
      "        [0.3855, 0.7981, 0.6199],\n",
      "        [0.1671, 0.1709, 0.2392],\n",
      "        [0.5782, 0.3083, 0.5117],\n",
      "        [0.9081, 0.6357, 0.3619],\n",
      "        [0.0705, 0.4811, 0.3636],\n",
      "        [0.2481, 0.5198, 0.2635]]), batch_sizes=tensor([3, 3, 2, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(\"Padded Seqs:\")\n",
    "print(pad_seq)\n",
    "print(\"Lens:\", lens)\n",
    "\n",
    "print(\"Pack Padded Seqs:\")\n",
    "pac_pad_seq = torch.nn.utils.rnn.pack_padded_sequence(pad_seq, lens, batch_first=True)\n",
    "print(pac_pad_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this is doing some magic -- getting rid of all padded zeros -- Cool!\n",
    "`batch_sizes=tensor([3, 3, 2, 1]` seems to be the main ingredient of this magic.\n",
    "\n",
    "`[3, 3, 2, 1]` I get it!\n",
    "We have 4 time steps in batch. \n",
    "- First two step has all 3 seqs in the batch. \n",
    "- third step is made of first 2 seqs in batch. \n",
    "- Fourth step is made of first seq in batch\n",
    "\n",
    "I now understand why the sequences in the batch has to be sorted by descending order of lengths!\n",
    "\n",
    "Now let us send it to LSTM and see what it produces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([[ 0.0428, -0.3015,  0.0359,  0.0557],\n",
      "        [ 0.0587, -0.3025,  0.0026,  0.0203],\n",
      "        [ 0.0147, -0.2585, -0.0057,  0.0754],\n",
      "        [ 0.0919, -0.4145,  0.0278,  0.0480],\n",
      "        [ 0.0537, -0.3388, -0.0491,  0.0110],\n",
      "        [ 0.0398, -0.3531, -0.0005,  0.0337],\n",
      "        [ 0.0768, -0.4989,  0.0203,  0.0674],\n",
      "        [ 0.0839, -0.3811, -0.0262, -0.0056],\n",
      "        [ 0.1019, -0.4925, -0.0177,  0.0224]], grad_fn=<CatBackward>), batch_sizes=tensor([3, 3, 2, 1]))\n"
     ]
    }
   ],
   "source": [
    "pac_pad_out, (pac_ht, pac_ct) = lstm(pac_pad_seq)\n",
    "# Lets first look at output. this is packed output\n",
    "print(pac_pad_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay this is packed output. Sequences are of unequal lengths.\n",
    "Now we need to restore the output by padding 0s for shorter sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.0428, -0.3015,  0.0359,  0.0557],\n",
      "         [ 0.0919, -0.4145,  0.0278,  0.0480],\n",
      "         [ 0.0768, -0.4989,  0.0203,  0.0674],\n",
      "         [ 0.1019, -0.4925, -0.0177,  0.0224]],\n",
      "\n",
      "        [[ 0.0587, -0.3025,  0.0026,  0.0203],\n",
      "         [ 0.0537, -0.3388, -0.0491,  0.0110],\n",
      "         [ 0.0839, -0.3811, -0.0262, -0.0056],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0147, -0.2585, -0.0057,  0.0754],\n",
      "         [ 0.0398, -0.3531, -0.0005,  0.0337],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=<TransposeBackward0>), tensor([4, 3, 2]))\n"
     ]
    }
   ],
   "source": [
    "pad_out = nn.utils.rnn.pad_packed_sequence(pac_pad_out, batch_first=True, padding_value=0)\n",
    "print(pad_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output looks good! Now Let us look at the hidden state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1019, -0.4925],\n",
      "         [ 0.0839, -0.3811],\n",
      "         [ 0.0398, -0.3531]],\n",
      "\n",
      "        [[ 0.0359,  0.0557],\n",
      "         [ 0.0026,  0.0203],\n",
      "         [-0.0057,  0.0754]]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(pac_ht)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great. As we see the forward (or Left-to-right) LSTM's last hidden state is proper as per the lengths. So should be the c_t.\n",
    "\n",
    "Let us concatenate forward and reverse LSTM's hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1019, -0.4925,  0.0359,  0.0557],\n",
       "        [ 0.0839, -0.3811,  0.0026,  0.0203],\n",
       "        [ 0.0398, -0.3531, -0.0057,  0.0754]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([pac_ht[0],pac_ht[1]], dim=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Multi Layer LSTM\n",
    "\n",
    "Let us redo the above hacking to understand how 2 layer LSTM works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 2\n",
    "inp_size = 3\n",
    "out_size = 2\n",
    "lstm2 = nn.LSTM(inp_size, out_size, num_layers=n_layers, batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packed Output:\n",
      "PackedSequence(data=tensor([[ 0.2443,  0.0703, -0.0871, -0.0664],\n",
      "        [ 0.2496,  0.0677, -0.0658, -0.0605],\n",
      "        [ 0.2419,  0.0687, -0.0701, -0.0521],\n",
      "        [ 0.3354,  0.0964, -0.0772, -0.0613],\n",
      "        [ 0.3272,  0.0975, -0.0655, -0.0534],\n",
      "        [ 0.3216,  0.1055, -0.0504, -0.0353],\n",
      "        [ 0.3644,  0.1065, -0.0752, -0.0531],\n",
      "        [ 0.3583,  0.1116, -0.0418, -0.0350],\n",
      "        [ 0.3760,  0.1139, -0.0438, -0.0351]], grad_fn=<CatBackward>), batch_sizes=tensor([3, 3, 2, 1]))\n",
      "Pad Output:\n",
      "(tensor([[[ 0.2443,  0.0703, -0.0871, -0.0664],\n",
      "         [ 0.3354,  0.0964, -0.0772, -0.0613],\n",
      "         [ 0.3644,  0.1065, -0.0752, -0.0531],\n",
      "         [ 0.3760,  0.1139, -0.0438, -0.0351]],\n",
      "\n",
      "        [[ 0.2496,  0.0677, -0.0658, -0.0605],\n",
      "         [ 0.3272,  0.0975, -0.0655, -0.0534],\n",
      "         [ 0.3583,  0.1116, -0.0418, -0.0350],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.2419,  0.0687, -0.0701, -0.0521],\n",
      "         [ 0.3216,  0.1055, -0.0504, -0.0353],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=<TransposeBackward0>), tensor([4, 3, 2]))\n",
      "Last h_n:\n",
      "tensor([[[ 0.2190,  0.2067],\n",
      "         [ 0.1868,  0.2188],\n",
      "         [ 0.1706,  0.2347]],\n",
      "\n",
      "        [[-0.5062,  0.1701],\n",
      "         [-0.4130,  0.2190],\n",
      "         [-0.4228,  0.1733]],\n",
      "\n",
      "        [[ 0.3760,  0.1139],\n",
      "         [ 0.3583,  0.1116],\n",
      "         [ 0.3216,  0.1055]],\n",
      "\n",
      "        [[-0.0871, -0.0664],\n",
      "         [-0.0658, -0.0605],\n",
      "         [-0.0701, -0.0521]]], grad_fn=<ViewBackward>)\n",
      "Last c_n:\n",
      "tensor([[[ 0.5656,  0.3145],\n",
      "         [ 0.4853,  0.3633],\n",
      "         [ 0.4255,  0.3718]],\n",
      "\n",
      "        [[-0.9779,  0.6461],\n",
      "         [-0.8578,  0.7013],\n",
      "         [-0.6978,  0.5322]],\n",
      "\n",
      "        [[ 1.0754,  0.4258],\n",
      "         [ 1.0021,  0.4184],\n",
      "         [ 0.8623,  0.3839]],\n",
      "\n",
      "        [[-0.1535, -0.2073],\n",
      "         [-0.1187, -0.1912],\n",
      "         [-0.1211, -0.1589]]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "pac_out, (h_n, c_n) = lstm2(pac_pad_seq)\n",
    "print(\"Packed Output:\")\n",
    "print(pac_out)\n",
    "pad_out = nn.utils.rnn.pad_packed_sequence(pac_out, batch_first=True, padding_value=0)\n",
    "print(\"Pad Output:\")\n",
    "print(pad_out)\n",
    "\n",
    "\n",
    "print(\"Last h_n:\")\n",
    "print(h_n)\n",
    "\n",
    "print(\"Last c_n:\")\n",
    "print(c_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM output looks similar to single layer LSTM.\n",
    "\n",
    "However the ht and ct states are bigger -- since there are two layers. \n",
    "Now its time to RTFM. \n",
    "\n",
    "\n",
    "> h_n of shape `(num_layers * num_directions, batch, hidden_size)`: tensor containing the hidden state for `t = seq_len`.\n",
    "Like output, the layers can be separated using `h_n.view(num_layers, num_directions, batch, hidden_size)` and similarly for c_n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3760,  0.1139],\n",
      "         [ 0.3583,  0.1116],\n",
      "         [ 0.3216,  0.1055]],\n",
      "\n",
      "        [[-0.0871, -0.0664],\n",
      "         [-0.0658, -0.0605],\n",
      "         [-0.0701, -0.0521]]], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "num_dirs = 2\n",
    "l_n_h_n = h_n.view(n_layers, num_dirs, batch_size, out_size)[-1]\n",
    "# last layer last time step hidden state\n",
    "print(l_n_h_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last layer last time stamp hidden state\n",
      "tensor([[ 0.3760,  0.1139, -0.0871, -0.0664],\n",
      "        [ 0.3583,  0.1116, -0.0658, -0.0605],\n",
      "        [ 0.3216,  0.1055, -0.0701, -0.0521]], grad_fn=<CatBackward>)\n",
      "Padded Outputs :\n",
      "(tensor([[[ 0.2443,  0.0703, -0.0871, -0.0664],\n",
      "         [ 0.3354,  0.0964, -0.0772, -0.0613],\n",
      "         [ 0.3644,  0.1065, -0.0752, -0.0531],\n",
      "         [ 0.3760,  0.1139, -0.0438, -0.0351]],\n",
      "\n",
      "        [[ 0.2496,  0.0677, -0.0658, -0.0605],\n",
      "         [ 0.3272,  0.0975, -0.0655, -0.0534],\n",
      "         [ 0.3583,  0.1116, -0.0418, -0.0350],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.2419,  0.0687, -0.0701, -0.0521],\n",
      "         [ 0.3216,  0.1055, -0.0504, -0.0353],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=<TransposeBackward0>), tensor([4, 3, 2]))\n"
     ]
    }
   ],
   "source": [
    "last_hid = torch.cat([l_n_h_n[0], l_n_h_n[1]], dim=1)\n",
    "\n",
    "print(\"last layer last time stamp hidden state\")\n",
    "print(last_hid)\n",
    "\n",
    "print(\"Padded Outputs :\")\n",
    "print(pad_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
