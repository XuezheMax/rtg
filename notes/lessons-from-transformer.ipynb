{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lessons Learned from Transformer Paper\n",
    "\n",
    "[\"Attention Is All You Need\"](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf), as cool as its title, is a vey nicely written paper.\n",
    "\n",
    "The [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) is an excellent guide to thoroughly understand it, to the level of implementation details.\n",
    "\n",
    "\n",
    "This paper pulled many useful independent research and put them together under one system.\n",
    "\n",
    "Here is comparision betweebn a`Beginner` (I when started studying NMT/seq2seq models) and I after reimplementing the `Transformer` paper build and train NMT models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|  Aspect                  |  Beginner             |  Transformer              |\n",
    "|--------------------------|-----------------------|---------------------------|\n",
    "| How you treat Words      | Each word is a vector | BPE to split subwords     |\n",
    "| How many vocabularies    | 2,different source and target | Single Shared Vocabulary |\n",
    "| How many Embeddings      | 3 different embedding     | Three way tied to a single matrix |\n",
    "| How to compute Loss      | Cross Entropy             | KL divergence              |\n",
    "| How to measure training time | Epoch based           | Step based training        |\n",
    "| Optimizer: How to set learning rate | Let ADAM do it | Increase 'til warmup steps, then decrease |\n",
    "| Attn: Decoder-to-Encoder | Maybe single Attention?   | Multiple Attentions       |\n",
    "| Attn: Encoder self attn  |  ?                   | Multiple Self Attentions  |\n",
    "| Attn: Decoder self attn  |  ?                   | Multiple Self Attentions  |\n",
    "| Residual connections     |  ?                   | Yes                       |\n",
    "| Normalizations           |  ?                   | Yes, Layer Normalization, embedding normalization |\n",
    "| Regularization: Label Smoothing |  ?            | Yes, smooth the labels         |\n",
    "| Regularization: Dropout  |  Maybe               | Yes, embedding, residual, attns |\n",
    "| Ensemble models          |  ?                   | Average last k check points   |  \n",
    "| Decoder                  | Beam decoder          | Beam decoder with length penalty|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
